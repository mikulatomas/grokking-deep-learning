{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_mnist import init, load\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n",
      "(1000, 10)\n",
      "(10000, 28, 28)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# in case that dataset is missing or pickle is broken uncomment this\n",
    "# init()\n",
    "\n",
    "x_train, y_train, x_test, y_test = load()\n",
    "\n",
    "# take first 1000 samples\n",
    "x_train = x_train[0:1000]\n",
    "y_train = y_train[0:1000]\n",
    "\n",
    "# transform labels from [2] to [0,0,1,0,0,0,0,0,0,0]\n",
    "OUT_CLASSES = 10\n",
    "\n",
    "transformed_y_train = []\n",
    "\n",
    "for y_label in y_train:\n",
    "    zero = np.zeros((OUT_CLASSES,))\n",
    "    zero[y_label] = 1\n",
    "    transformed_y_train.append(zero)\n",
    "\n",
    "y_train = np.array(transformed_y_train)\n",
    "\n",
    "transformed_y_test = []\n",
    "\n",
    "for y_label in y_test:\n",
    "    zero = np.zeros((OUT_CLASSES,))\n",
    "    zero[y_label] = 1\n",
    "    transformed_y_test.append(zero)\n",
    "\n",
    "y_test = np.array(transformed_y_test)\n",
    "\n",
    "# normalize input, avoid divergence\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# prepare input for conv layer\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 28, 28)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "def softmax2deriv(output):\n",
    "    return output / (output.shape[0] * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "ALPHA = 2\n",
    "EPOCHS = 300\n",
    "\n",
    "INPUT_ROWS = 28\n",
    "INPUT_COLUMNS = 28\n",
    "\n",
    "KERNEL_ROWS = 3\n",
    "KERNEL_COLUMNS = 3\n",
    "KERNEL_COUNT = 16\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "NUMBER_OF_SAMPLES_PER_IMAGE = (INPUT_ROWS - KERNEL_ROWS) * (INPUT_COLUMNS - KERNEL_COLUMNS)\n",
    "\n",
    "# (9, 16)\n",
    "kernels = 0.02 * np.random.rand(KERNEL_ROWS * KERNEL_COLUMNS, KERNEL_COUNT) - 0.01\n",
    "\n",
    "# (25*25*16, OUTPUT_SIZE)\n",
    "weights_1_2 = 0.2 * np.random.rand(NUMBER_OF_SAMPLES_PER_IMAGE * KERNEL_COUNT, OUTPUT_SIZE) - 0.1\n",
    "\n",
    "\n",
    "def get_sections_from_input(input_data):\n",
    "    sections = []\n",
    "    \n",
    "    for row in range(INPUT_ROWS - KERNEL_ROWS):\n",
    "        for column in range(INPUT_COLUMNS - KERNEL_COLUMNS):\n",
    "            section = input_data[:, row:row + KERNEL_ROWS, column:column + KERNEL_COLUMNS]\n",
    "            \n",
    "            # extend by one axis for future concatenation\n",
    "            sections.append(section.reshape(-1,1,KERNEL_ROWS, KERNEL_COLUMNS))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def predict(input_data, kernels, weights_1_2):\n",
    "    # list of len 25x25 of (128, 3, 3)\n",
    "    sections = get_sections_from_input(input_data)\n",
    "    \n",
    "    # (625, 128, 3, 3)\n",
    "    sections_array = np.array(sections)\n",
    "    \n",
    "    # reshape to (80 000, 9) 625*128 = 80k\n",
    "    input_flattened = sections_array.reshape(-1, KERNEL_ROWS * KERNEL_COLUMNS)\n",
    "    \n",
    "    # reshape to (128, 25*25*16)\n",
    "    layer_1 = input_flattened.dot(kernels).reshape(input_data.shape[0], -1)\n",
    "    layer_1 = tanh(layer_1)\n",
    "    \n",
    "#     dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "#     layer_1 = layer_1 * dropout_mask * 2\n",
    "    \n",
    "    layer_2 = layer_1.dot(weights_1_2)\n",
    "#     layer_2 = softmax(layer_2)\n",
    "    \n",
    "    return layer_2\n",
    "    \n",
    "def accuracy(x_input, y_input, kernels, weights_1_2):\n",
    "    prediction = predict(x_input, kernels, weights_1_2)\n",
    "    \n",
    "    match = 0\n",
    "    for predicted, label in zip(prediction, y_input):\n",
    "        if np.argmax(predicted) == np.argmax(label):\n",
    "            match += 1\n",
    "\n",
    "    return match / len(prediction)  \n",
    "\n",
    "def train(x_train, y_train, kernels, weights_1_2):\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        for batch_id in range(x_train.shape[0] // BATCH_SIZE):\n",
    "            batch_index_start = batch_id * BATCH_SIZE\n",
    "            batch_index_end = batch_index_start + BATCH_SIZE\n",
    "\n",
    "            # (128, 28, 28)\n",
    "            x_train_batch = x_train[batch_index_start:batch_index_end]\n",
    "            y_train_batch = y_train[batch_index_start:batch_index_end]\n",
    "\n",
    "            # list of len 25x25 of (128, 1, 3, 3)\n",
    "            sections = get_sections_from_input(x_train_batch)\n",
    "            \n",
    "            # concatenate sections by axis=1\n",
    "            # (128, 625, 3, 3)\n",
    "            sections_array = np.concatenate(sections, axis=1)\n",
    "\n",
    "            # reshape to (80 000, 9) 625*128 = 80k\n",
    "            input_flattened = sections_array.reshape(-1, KERNEL_ROWS * KERNEL_COLUMNS)\n",
    "            \n",
    "            # reshape to (128, 25*25*16)\n",
    "            layer_1 = input_flattened.dot(kernels).reshape(x_train_batch.shape[0], -1)\n",
    "            layer_1 = tanh(layer_1)\n",
    "\n",
    "            dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "            layer_1 = layer_1 * dropout_mask * 2\n",
    "\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "            layer_2 = softmax(layer_2)\n",
    "\n",
    "            # backpropagation\n",
    "            # BATCH_SIZE because delta is calculated from number of BATCH_SIZE samples\n",
    "            # (BATCH_SIZE, 10)\n",
    "            layer_2_delta = softmax2deriv((y_train_batch - layer_2))\n",
    "\n",
    "            # (128, 25*25*16)\n",
    "            layer_1_delta = layer_2_delta.dot(weights_1_2.T)\n",
    "            layer_1_delta = layer_1_delta * tanh2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "\n",
    "            # weighted delta - how much network misses because of wrong weights\n",
    "            # (25*25*16, OUTPUT_SIZE)\n",
    "            weighted_delta_1_2 = layer_1.T.dot(layer_2_delta)\n",
    "            weights_1_2 += ALPHA * weighted_delta_1_2\n",
    "\n",
    "            # weighted delta - how much network misses because of wrong weights\n",
    "            # (9, 80 000) * (80 000, 16)\n",
    "            weighted_delta_kernels = input_flattened.T.dot(layer_1_delta.reshape(-1, KERNEL_COUNT))\n",
    "            kernels += ALPHA * weighted_delta_kernels\n",
    "               \n",
    "        if epoch % 50 == 0:    \n",
    "            print(f\"Train acc: {accuracy(x_train, y_train, kernels, weights_1_2)}\")\n",
    "            print(f\"Test acc: {accuracy(x_test, y_test, kernels, weights_1_2)}\")\n",
    "        \n",
    "    return kernels, weights_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train acc: 0.107\n",
      "Test acc: 0.0992\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Train acc: 0.099\n",
      "Test acc: 0.0982\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e943f05cade0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_1_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-bb12ab3b94d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, y_train, kernels, weights_1_2)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# weighted delta - how much network misses because of wrong weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# (9, 80 000) * (80 000, 16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mweighted_delta_kernels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_flattened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_1_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKERNEL_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mkernels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mALPHA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweighted_delta_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kernels, weights_1_2 = train(x_train, y_train, kernels, weights_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import plot\n",
    "\n",
    "# plot history and see generalization\n",
    "plot(list(range(len(acc_history))), acc_history, '-')\n",
    "plot(list(range(len(test_history))), test_history, 'g-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000,28, 28) / 255,\n",
    "                  y_train[0:1000])\n",
    "\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28,28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "    \n",
    "x_train = images\n",
    "y_train = labels\n",
    "x_test = test_images\n",
    "y_test = test_labels\n",
    "# in case that dataset is missing or pickle is broken uncomment this\n",
    "# init()\n",
    "\n",
    "# x_train, y_train, x_test, y_test = load()\n",
    "\n",
    "# # take first 1000 samples\n",
    "# x_train = x_train[0:1000]\n",
    "# y_train = y_train[0:1000]\n",
    "\n",
    "# # transform labels from [2] to [0,0,1,0,0,0,0,0,0,0]\n",
    "# OUT_CLASSES = 10\n",
    "\n",
    "# transformed_y_train = []\n",
    "\n",
    "# for y_label in y_train:\n",
    "#     zero = np.zeros((OUT_CLASSES,))\n",
    "#     zero[y_label] = 1\n",
    "#     transformed_y_train.append(zero)\n",
    "\n",
    "# y_train = transformed_y_train\n",
    "\n",
    "# transformed_y_test = []\n",
    "\n",
    "# for y_label in y_test:\n",
    "#     zero = np.zeros((OUT_CLASSES,))\n",
    "#     zero[y_label] = 1\n",
    "#     transformed_y_test.append(zero)\n",
    "\n",
    "# y_test = transformed_y_test\n",
    "\n",
    "# # normalize input, avoid divergence\n",
    "# x_train = x_train / 255\n",
    "# x_test = x_test / 255\n",
    "\n",
    "# # prepare input for conv layer\n",
    "# x_train = x_train.reshape(x_train.shape[0], 28, 28)\n",
    "# x_test = x_test.reshape(x_test.shape[0], 28, 28)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "def softmax2deriv(output):\n",
    "    return output / (output.shape[0] * BATCH_SIZE)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "ALPHA = 2\n",
    "EPOCHS = 300\n",
    "\n",
    "INPUT_ROWS = 28\n",
    "INPUT_COLUMNS = 28\n",
    "\n",
    "KERNEL_ROWS = 3\n",
    "KERNEL_COLUMNS = 3\n",
    "KERNEL_COUNT = 16\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "NUMBER_OF_SAMPLES_PER_IMAGE = (INPUT_ROWS - KERNEL_ROWS) * (INPUT_COLUMNS - KERNEL_COLUMNS)\n",
    "\n",
    "# (9, 16)\n",
    "kernels = 0.02 * np.random.random((KERNEL_ROWS * KERNEL_COLUMNS, KERNEL_COUNT)) - 0.01\n",
    "\n",
    "# (25*25*16, OUTPUT_SIZE)\n",
    "weights_1_2 = 0.02 * np.random.random((NUMBER_OF_SAMPLES_PER_IMAGE * KERNEL_COUNT, OUTPUT_SIZE)) - 0.01\n",
    "\n",
    "\n",
    "def get_sections_from_input(input_data):\n",
    "    sections = []\n",
    "    \n",
    "    for row in range(INPUT_ROWS - KERNEL_ROWS):\n",
    "        for column in range(INPUT_COLUMNS - KERNEL_COLUMNS):\n",
    "            section = input_data[:, row:row + KERNEL_ROWS, column:column + KERNEL_COLUMNS]\n",
    "            \n",
    "            # extend by one axis for future concatenation\n",
    "            sections.append(section.reshape(-1,1,KERNEL_ROWS, KERNEL_COLUMNS))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def predict(input_data, kernels, weights_1_2):\n",
    "    # list of len 25x25 of (128, 3, 3)\n",
    "    sections = get_sections_from_input(input_data)\n",
    "            \n",
    "    # concatenate sections by axis=1\n",
    "    # (128, 625, 3, 3)\n",
    "    sections_array = np.concatenate(sections, axis=1)\n",
    "\n",
    "    # reshape to (80 000, 9) 625*128 = 80k\n",
    "    # input_flattened = sections_array.reshape(-1, KERNEL_ROWS * KERNEL_COLUMNS)\n",
    "    s = sections_array.shape\n",
    "    input_flattened = sections_array.reshape(s[0]*s[1], -1)\n",
    "\n",
    "    # reshape to (128, 25*25*16)\n",
    "    # layer_1 = input_flattened.dot(kernels).reshape(x_train_batch.shape[0], -1)\n",
    "    layer_1 = input_flattened.dot(kernels).reshape(s[0], -1)\n",
    "    layer_1 = tanh(layer_1)\n",
    "\n",
    "    dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "    layer_1 *= dropout_mask * 2\n",
    "\n",
    "    layer_2 = layer_1.dot(weights_1_2)\n",
    "    layer_2 = softmax(layer_2)\n",
    "    \n",
    "    return layer_2\n",
    "    \n",
    "def accuracy(x_input, y_input, kernels, weights_1_2):\n",
    "    prediction = predict(x_input, kernels, weights_1_2)\n",
    "    \n",
    "    match = 0\n",
    "    for predicted, label in zip(prediction, y_input):\n",
    "        if np.argmax(predicted) == np.argmax(label):\n",
    "            match += 1\n",
    "\n",
    "    return match / len(prediction)  \n",
    "\n",
    "def train(x_train, y_train, kernels, weights_1_2):\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        for batch_id in range(x_train.shape[0] // BATCH_SIZE):\n",
    "            batch_index_start = batch_id * BATCH_SIZE\n",
    "            batch_index_end = batch_index_start + BATCH_SIZE\n",
    "\n",
    "            # (128, 28, 28)\n",
    "            x_train_batch = x_train[batch_index_start:batch_index_end]\n",
    "            y_train_batch = y_train[batch_index_start:batch_index_end]\n",
    "\n",
    "            # list of len 25x25 of (128, 1, 3, 3)\n",
    "            sections = get_sections_from_input(x_train_batch)\n",
    "            \n",
    "            # concatenate sections by axis=1\n",
    "            # (128, 625, 3, 3)\n",
    "            sections_array = np.concatenate(sections, axis=1)\n",
    "\n",
    "            # reshape to (80 000, 9) 625*128 = 80k\n",
    "            # input_flattened = sections_array.reshape(-1, KERNEL_ROWS * KERNEL_COLUMNS)\n",
    "            s = sections_array.shape\n",
    "            input_flattened = sections_array.reshape(s[0]*s[1], -1)\n",
    "            \n",
    "            # reshape to (128, 25*25*16)\n",
    "            # layer_1 = input_flattened.dot(kernels).reshape(x_train_batch.shape[0], -1)\n",
    "            kernel_output = input_flattened.dot(kernels)\n",
    "            layer_1 = kernel_output.reshape(s[0], -1)\n",
    "            layer_1 = tanh(layer_1)\n",
    "\n",
    "            dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "            layer_1 *= dropout_mask * 2\n",
    "\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "            layer_2 = softmax(layer_2)\n",
    "\n",
    "            # backpropagation\n",
    "            # BATCH_SIZE because delta is calculated from number of BATCH_SIZE samples\n",
    "            # (BATCH_SIZE, 10)\n",
    "            # layer_2_delta = softmax2deriv((y_train_batch - layer_2))\n",
    "            layer_2_delta = (y_train_batch - layer_2)\\\n",
    "                        / (BATCH_SIZE * layer_2.shape[0])\n",
    "\n",
    "            # (128, 25*25*16)\n",
    "            layer_1_delta = layer_2_delta.dot(weights_1_2.T)\n",
    "            layer_1_delta = layer_1_delta * tanh2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "\n",
    "            # weighted delta - how much network misses because of wrong weights\n",
    "            # (25*25*16, OUTPUT_SIZE)\n",
    "            weighted_delta_1_2 = layer_1.T.dot(layer_2_delta)\n",
    "            weights_1_2 += ALPHA * weighted_delta_1_2\n",
    "\n",
    "            # weighted delta - how much network misses because of wrong weights\n",
    "            # (9, 80 000) * (80 000, 16)\n",
    "            weighted_delta_kernels = input_flattened.T.dot(layer_1_delta.reshape(kernel_output.shape))\n",
    "            kernels -= ALPHA * weighted_delta_kernels\n",
    "               \n",
    "        if epoch % 50 == 0:    \n",
    "            print(f\"Train acc: {accuracy(x_train, y_train, kernels, weights_1_2)}\")\n",
    "            print(f\"Test acc: {accuracy(x_test, y_test, kernels, weights_1_2)}\")\n",
    "        \n",
    "    return kernels, weights_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train acc: 0.084\n",
      "Test acc: 0.0837\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Train acc: 0.15\n",
      "Test acc: 0.1299\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Train acc: 0.1\n",
      "Test acc: 0.0907\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e943f05cade0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_1_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-750b05800a04>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, y_train, kernels, weights_1_2)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# weighted delta - how much network misses because of wrong weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# (25*25*16, OUTPUT_SIZE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mweighted_delta_1_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_2_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mweights_1_2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mALPHA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweighted_delta_1_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kernels, weights_1_2 = train(x_train, y_train, kernels, weights_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(1000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255,\n",
    "                  y_train[0:1000])\n",
    "\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8e0fcd6aa4cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from init_mnist import init, load\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "x_train, y_train, x_test, y_test = load()\n",
    "\n",
    "# take first 1000 samples\n",
    "x_train = x_train[0:1000]\n",
    "y_train = y_train[0:1000]\n",
    "\n",
    "# transform labels from [2] to [0,0,1,0,0,0,0,0,0,0]\n",
    "OUT_CLASSES = 10\n",
    "\n",
    "transformed_y_train = []\n",
    "\n",
    "for y_label in y_train:\n",
    "    zero = np.zeros((OUT_CLASSES,))\n",
    "    zero[y_label] = 1\n",
    "    transformed_y_train.append(zero)\n",
    "\n",
    "y_train = transformed_y_train\n",
    "\n",
    "transformed_y_test = []\n",
    "\n",
    "for y_label in y_test:\n",
    "    zero = np.zeros((OUT_CLASSES,))\n",
    "    zero[y_label] = 1\n",
    "    transformed_y_test.append(zero)\n",
    "\n",
    "y_test = transformed_y_test\n",
    "\n",
    "# normalize input, avoid divergence\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# prepare input for conv layer\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
