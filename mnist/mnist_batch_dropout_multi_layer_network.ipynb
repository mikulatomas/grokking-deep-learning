{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_mnist import init, load\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case that dataset is missing or pickle is broken uncomment this\n",
    "# init()\n",
    "\n",
    "x_train, y_train, x_test, y_test = load()\n",
    "\n",
    "# take first 1000 samples\n",
    "x_train = x_train[0:1000]\n",
    "y_train = y_train[0:1000]\n",
    "\n",
    "# transform labels from [2] to [0,0,1,0,0,0,0,0,0,0]\n",
    "OUT_CLASSES = 10\n",
    "\n",
    "transformed_y_train = []\n",
    "\n",
    "for y_label in y_train:\n",
    "    zero = np.zeros((OUT_CLASSES,))\n",
    "    zero[y_label] = 1\n",
    "    transformed_y_train.append(zero)\n",
    "\n",
    "y_train = transformed_y_train\n",
    "\n",
    "transformed_y_test = []\n",
    "\n",
    "for y_label in y_test:\n",
    "    zero = np.zeros((OUT_CLASSES,))\n",
    "    zero[y_label] = 1\n",
    "    transformed_y_test.append(zero)\n",
    "\n",
    "y_test = transformed_y_test\n",
    "\n",
    "# normalize input, avoid divergence\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return x > 0\n",
    "\n",
    "def predict(input_data, weights):\n",
    "    # input dot weights between 0 and 1 layer\n",
    "    # (1, 783) dot (738, 30) -> (1, 30)\n",
    "    layer_1 = relu(np.dot(input_data, weights[0]))\n",
    "    # output from layer 1 dot weights between 1 and 2 layer\n",
    "    # (1, 30) dot (30, 10) -> (1, 10)\n",
    "    layer_2 = np.dot(layer_1, weights[1])\n",
    "    \n",
    "    return layer_1, layer_2\n",
    "\n",
    "# calculate accuracy\n",
    "def accuracy(x_train, y_train, weights):\n",
    "    match = 0\n",
    "    for input_, label in zip(x_train, y_train):\n",
    "        _, output = predict(input_, weights)\n",
    "        \n",
    "        if np.argmax(output) == np.argmax(label):\n",
    "            match += 1\n",
    "\n",
    "    return match / len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10862357,  0.11626779, -0.02466919, -0.22713466, -0.00352367,\n",
       "        0.12664767,  0.00669752,  0.19087953,  0.57041257, -0.04134471])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALPHA = 0.1\n",
    "EPOCHS = 200\n",
    "# size of the batch\n",
    "BATCH_SIZE = 100\n",
    "# input layer\n",
    "LAYER_0_NODES = x_train.shape[1]\n",
    "# hidden layer\n",
    "LAYER_1_NODES = 100\n",
    "# output layer\n",
    "LAYER_2_NODES = 10\n",
    "\n",
    "# init weights (-0.1 to 0.1 range)\n",
    "init_weights_0_1 = 0.2 * np.random.rand(LAYER_0_NODES, LAYER_1_NODES) - 0.1\n",
    "init_weights_1_2 = 0.2 * np.random.rand(LAYER_1_NODES, LAYER_2_NODES) - 0.1\n",
    "\n",
    "# random prediction\n",
    "_ , result = predict(x_train[0], (init_weights_0_1, init_weights_1_2))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, train_labels, init_weights, alpha, number_of_epoch):\n",
    "    weights_0_1 = init_weights[0]\n",
    "    weights_1_2 = init_weights[1]\n",
    "    \n",
    "    print(\"Epoch:\", end = '')\n",
    "    \n",
    "    acc_history = [accuracy(x_train, y_train, (weights_0_1, weights_1_2))]\n",
    "    test_history = [accuracy(x_test, y_test, (weights_0_1, weights_1_2))]\n",
    "    \n",
    "    for i in range(number_of_epoch):\n",
    "        print(\".\", end = '')\n",
    "        \n",
    "        for batch_number in range(int(len(x_train) / BATCH_SIZE)):\n",
    "            batch_start, batch_end = batch_number * BATCH_SIZE, (batch_number + 1) * BATCH_SIZE\n",
    "            \n",
    "            # input layer\n",
    "            layer_0 = x_train[batch_start:batch_end]\n",
    "            \n",
    "            # input dot weights between 0 and 1 layer\n",
    "            # (BATCH_SIZE, 783) dot (738, 30) -> (BATCH_SIZE, 30)\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            \n",
    "            # dropout layer_1\n",
    "            dropout_mask = np.random.randint(2, size=len(layer_1))\n",
    "            layer_1 *= dropout_mask * 2\n",
    "            \n",
    "            # output from layer 1 dot weights between 1 and 2 layer\n",
    "            # (BATCH_SIZE, 30) dot (30, 10) -> (BATCH_SIZE, 10)\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "            \n",
    "            # delta between prediction and expected result\n",
    "            # (BATCH_SIZE, 10) - (BATCH_SIZE, 10) -> (BATCH_SIZE, 10)\n",
    "            delta_layer_2 = (layer_2 - y_train[batch_start:batch_end]) / BATCH_SIZE\n",
    "\n",
    "            # delta on hidden layer, multiply output delta by weights between 1 and 2 layer\n",
    "            # (BATCH_SIZE, 10) dot (10, 30) * (BATCH_SIZE, 30) -> (BATCH_SIZE, 30)\n",
    "            delta_layer_1 = delta_layer_2.dot(weights_1_2.T) * relu_deriv(layer_1)\n",
    "\n",
    "            # apply dropout to delta\n",
    "            delta_layer_1 *= dropout_mask\n",
    "\n",
    "            # do the learning (backpropagation)\n",
    "            # alpha * (30, BATCH_SIZE) dot (BATCH_SIZE, 10) -> (30, 10)\n",
    "            weights_1_2 = weights_1_2 - alpha * np.atleast_2d(layer_1).T.dot(np.atleast_2d(delta_layer_2))\n",
    "            # alpha * (783, BATCH_SIZE) dot (BATCH_SIZE, 30) -> (783, 30)\n",
    "            weights_0_1 = weights_0_1 - alpha * np.atleast_2d(layer_0).T.dot(np.atleast_2d(delta_layer_1))\n",
    "    \n",
    "        acc_history.append(accuracy(x_train, y_train, (weights_0_1, weights_1_2)))\n",
    "        test_history.append(accuracy(x_test, y_test, (weights_0_1, weights_1_2)))\n",
    "\n",
    "\n",
    "    return (weights_0_1, weights_1_2), acc_history, test_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:................................................................"
     ]
    }
   ],
   "source": [
    "weights, acc_history, test_history = train(x_train, y_train, (init_weights_0_1, init_weights_1_2), ALPHA, EPOCHS)\n",
    "\n",
    "(acc_history[-1], test_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import plot\n",
    "\n",
    "# plot history and see generalization\n",
    "plot(list(range(len(acc_history))), acc_history, '-')\n",
    "plot(list(range(len(test_history))), test_history, 'g-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
